{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Stanislav Borysov [stabo@dtu.dk], DTU Management*\n",
    "# Advanced Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshing Python and Machine Learning: Part 3 - Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on the notebooks from 42184 Data Science for Mobility E19 / 42577 Introduction to Business Analytics E19*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the previous notebook you learned about Numpy. You probably felt it to be sometimes complicated for routine tasks (e.g. get only data from one of the fields of a file, convert individual fields). \n",
    "\n",
    "The Pandas module tries to automatize a series of typical tasks that us, Data Scientists, have to do all the time. This doesn't mean that what you learned with Numpy is not useful anymore. In fact, all of that will be precious in many circumstances (e.g. Pandas uses a lot of Numpy features too!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview: Data frames - a powerful data-structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas module provides a powerful data-structure called a data frame.\n",
    "\n",
    "It is similar, but not identical to:\n",
    "* a table in a relational database,\n",
    "* an Excel spreadsheet,\n",
    "* a dataframe in R.\n",
    "\n",
    "Data frames can be read and written to/from:\n",
    "* database queries, database tables\n",
    "* CSV files\n",
    "* json files\n",
    "\n",
    "Beware that data frames are memory resident!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pandas\n",
    "\n",
    "The pandas module is usually imported with the alias pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series - DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is object-oriented. We create data frames by constructing instances of different classes. The two most important classes are:\n",
    "\n",
    "* `DataFrame`\n",
    "* `Series`\n",
    "\n",
    "Let's create our first `Series`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Don't forget to play with our code to really understand what's going on!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.random.randn(5)    #create 5 random numbers\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series = pd.Series(data, index=['a', 'b', 'c', 'd', 'e'])\n",
    "my_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **plot** a `Series` by invoking the `plot()` method on an instance of a `Series` object. The x-axis will automatically be labelled with the series index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "my_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a series with automatic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series can be accessed using the same syntax as arrays and dicts. So, we are going to use the labels in the index to access each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the label like an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can specify a range of labels to obtain a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series[['b', 'c']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a **DataFrame** by passing a numpy array, with a datetime index and labeled columns.\n",
    "\n",
    "First, the datetime index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['A', 'B', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a second `DataFrame` by passing a dict of objects that can be converted to series-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({ 'A' : [1., 2.1, 3.5, 8.1],\n",
    "                    'B' : pd.Timestamp('20130102'),\n",
    "                    'C' : pd.Series(1,index=list(range(4)),dtype='float32')})\n",
    "print(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the top & bottom rows of the frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 top rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 last rows\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the **index, columns, and the underlying numpy data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in seeing quick statistic summary of your data then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if you want to sort your data by a certain column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to select a specific column to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['A lk afg a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can be selected using either the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['20130102':'20130104'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or simpler using the numeric index of the rows you want to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting a cross section using a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[dates[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be even more specific and show only the first two columns of the first three rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['20130101':'20130103',['A','B']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we do not care about the actual value of the index, but only its relevant position. Therefore, if we want to print the values of the fourth date-row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By integer slices, acting similar to numpy/python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[3:5,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting tool is **the boolean indexing**. We can isolate the rows that meet a specific criterion that we want, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.A > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For filtering, the following command helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's create a copy of our dataframe\n",
    "df2 = df.copy()\n",
    "\n",
    "# Now, we will add a new column, named \"E\" with the following values:\n",
    "df2['E'] = 'four'\n",
    "\n",
    "#So, now if we want to show the rows that have the value \"two\" or \"four\" at the column \"E\", we will type: \n",
    "df2[df2['E'].isin(['two','four'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['a', 'b', 'c']\n",
    "b = 'b'\n",
    "b in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations \n",
    "\n",
    "One of the essential pieces of NumPy is the ability to perform quick elementwise operations, both with basic arithmetic (addition, subtraction, multiplication, etc.) and with more sophisticated operations (trigonometric functions, exponential and logarithmic functions, etc.). Pandas inherits much of this functionality from NumPy, and the universal functions are key to this.\n",
    "\n",
    "Pandas includes a couple useful twists, however: for unary operations like negation and trigonometric functions, these universal functions will preserve index and column labels in the output, and for binary operations such as addition and multiplication, Pandas will automatically align indices when passing the objects to the universal function. This means that keeping the context of data, and combining data from different sources â€“ both potentially error-prone tasks with raw NumPy arrays â€“ become essentially foolproof with Pandas. We will additionally see that there are well-defined operations between one-dimensional `Series` structures and two-dimensional `DataFrame` structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Pandas is designed to work with NumPy, any NumPy  universal function will work on pandas Series and DataFrame objects. Lets start by defining a simple Series and DataFrame on which to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)), columns=['A', 'B', 'C', 'D'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply a NumPy universal function on either of these objects, the result will be another Pandas object with the indices preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, for a slightly more complex calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(df * np.pi / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary operations on two `Series` or `DataFrame` objects, Pandas will align indices in the process of performing the operation. This is very convenient when working with **incomplete data**, as weâ€™ll see in some of the examples below.\n",
    "\n",
    "As an example, suppose we are combinging two different data sources, and find only the top three US states by area and the top three US states by population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "area = pd.Series({'Alaska': 1723337, 'Texas': 695662,'California': 423967}, name='area')\n",
    "population = pd.Series({'California': 38332521, 'Texas': 26448193,'New York': 19651127}, name='population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s see what happens when we divide these to compute the population density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population / area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting array contains the **union** of indices of the two input arrays, which could be determined using standard Python set arithmetic on these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area.index | population.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any item for which one or the other does not have an entry is marked by NaN, or â€œNot a Numberâ€, which is how Pandas marks missing data. This index matching is implemented this way for any of Pythons built-in arithmetic expressions; any missing values are filled-in with NaN by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.Series([2, 4, 6], index=[0, 1, 2])\n",
    "B = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If filling-in NaN values is not the desired behavior, the fill value can be modified using appropriate object methods in place of the operators. For example, calling `A.add(B)` is equivalent to calling A + B, but allows optional explicit specification of the fill value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.add(B, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar type of alingment takes place for both columns and indices when performing operations on dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(rng.randint(0, 20, (2, 2)), columns=list('AB'))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pd.DataFrame(rng.randint(0, 10, (3, 3)),columns=list('BAC'))\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that indices are aligned correctly irrespective of their order in the two objects, and indices in the result are sorted. Similarly to the case of the Series, we can use the associated objectâ€™s arithmetic method and pass any desired fill_value to be used in place of missing entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.add(B, fill_value=np.mean(A.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, Join, Append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10, 4))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break it into pieces\n",
    "pieces = [df[:3], df[3:7], df[7:]]\n",
    "pieces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`pandas.concat`** command concatenates pandas objects along a particular axis with optional set logic along the other axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat(pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataFrame.merge`** command merges DataFrame objects by performing a database-style join operation by columns or indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})\n",
    "right = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})\n",
    "\n",
    "pd.merge(left, right, on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataFrame.join`** command joins columns with other DataFrame either on index or on a key column. Efficiently Join multiple DataFrame objects by index at once by passing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, lsuffix='_caller', rsuffix='_other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DataFrame.append`** command appends rows of other to the end of this frame, returning a new object. Columns not in this frame are added as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.append(pieces[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you have a set of data that lends itself to being categorized or grouped. As a general example, let's say we have data on a wide variety of people. We may perform an analysis where we compare groups in the data based on age, gender, birth month, shoe size, or birth city; the options are as numerous as the data points!\n",
    "\n",
    "I think it would be intriguing to work now with a proper dataset from Citi Bike - Daily Ridership and Membership Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/201707-citibike-tripdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a good opportunity to break the data down into groups to look for some interesting trends. Some ideas are:\n",
    "\n",
    "* Group on the gender column and see if there are more male or female riders.\n",
    "* Do specific stations get used more than others? We can group on the station start or finish id.\n",
    "* Group the data on the day of the week, to see if there is more utilization for a particular day, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about a few examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to group by just the gender, then we pass this key (column name) to the groupby function as the sole argument. This example is the simplest form of grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedGender = df.groupby('gender')\n",
    "print(groupedGender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that groupby returns a pandas `DataFrameGroupBy` object. Pandas has just made some internal calculations about the new gender groups and is ready to apply some operation on each of these groups.\n",
    "\n",
    "Getting back to the data, if we use the `count` method, we can see the total number of entries **for each gender group**. For reference, here's what the website (from where the dataset was downloaded) says for the gender codes - \"Gender (Zero=unknown; 1=male; 2=female)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedGender.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedGender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the size as a percentage of the whole (using the trip)\n",
    "total = df.gender.count()\n",
    "groupedGender.size() / total * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like males make up the majority of Citi Bike riders (~63%)!\n",
    "\n",
    "We can use a single column from the `DataFrameGroupBy` object and apply some aggregation function on it - how about the **mean** and **standard deviation** of the trip durations for all three groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedGender['tripduration'].mean() / 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't have to use the bracket notation\n",
    "groupedGender.tripduration.std() / 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although males make up the majority of Citi Bike riders, there's not much of a difference in their trip durations. Interestingly, gender unknown riders take 2x as long of rides on average. These riders are likely single-use customers (when you purchase a one time pass at a Citi Bike kiosk you are not asked for your gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are some summary statistics for these groups (as an aside, you can use the describe function to get these statistics and more in one call). That's a whole lot of spread around the mean, which probably means there are some outliers in the data (maybe people that kept the bike for days). Just a brief look at this even though it's outside the scope, because I'm sure you were all interested ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.tripduration > 10000].tripduration.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our suspicions are confirmed - there are many bike rentals outside 2:45 even though the \"max\" is supposed to be 30 minutes (or 45 if you're a Citi Bike member)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "\n",
    "df_short = df[df.tripduration < 10000]\n",
    "df_short.tripduration = df_short.tripduration / 60.\n",
    "ggplot(df_short, aes(x='tripduration')) + geom_histogram(bins=30) + xlab(\"Trip Duration (mins)\") + ylab(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last example is looking at which are the five favorite start and end stations. We'll group the data based on the start and end station names, apply the count function, and sort the values is descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedStart = df.groupby('start station name')\n",
    "groupedStart['start station name'].count().sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupedEnd = df.groupby('end station name')\n",
    "groupedEnd['end station name'].count().sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing! Whenever we have a time-series dataset it is very useful to assign the time as index for our dataframe. Let's do that with the method **`set_index()`** of the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.set_index('starttime')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen today how we could identify demand patterns using the timestamps that the dataset offers. Next time, we will be able to see more using the Python Pandas module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore in more depth the possibilities providaded by the Python Pandas module. To make things less confusing, we decided to use the same dataset as in the numpy exercise. In fact, you'll start with doing exactly the **same** exercises. Then, an incremental set of exercises end up with ... !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start. Since we are going to work with Pandas, let's just import it, shall we (you'll eventually also need Numpy, so import that too)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some data. Please open the file \"pickups_zone_1_15min.csv\". This corresponds to the series of taxi-pickups in New York zone 1 (an area in the Manhattan island). \n",
    "\n",
    "You can use the method read_csv(file), which **returns** a DataFrame:\n",
    "\n",
    "> df=pd.read_csv(file)\n",
    "\n",
    "df is a variable that now has a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so we just loaded the file, now let's look at its content (and keep remembering how more complicated it was in Numpy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let's check how many lines the file actually has (now it should be 262848, one line less, why?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 lines of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, remember that we need to make a single field with the datetime (instead of 3 separate ones...)!\n",
    "\n",
    "Pandas actually allows us to simplify that right when we load the file. So, let's do it again, but now take a look at the following Stackexchange thread: https://stackoverflow.com/questions/38509512/pandas-read-csv-with-date-in-2-columns\n",
    "\n",
    "How about applying this here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may happen that the (new) temporal field is still a string and not a datetime object. Can you correct that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it can be useful to assign the time as index for our dataframe. Let's do that with the method set_index() of the DataFrame object. Notice that it doesn't do \"in-place\" changes, in other words, this command would not work: \n",
    "> df.set_index(KEY)\n",
    "\n",
    "You'd have to do\n",
    "\n",
    ">df=df.set_index(KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: You can always get a vector with the indeces themselves with \n",
    "> df.index\n",
    "\n",
    "To understand this, just try it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, just for fun (and to compare with the amount of work we did with Numpy), let's get **only** the pickups part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trivial, isn't it? It's even indexed correctly! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for our histogram for the pickups. If you can't remember or don't know how it's done, feel free to search online (something like \"Pandas histogram\")\n",
    "\n",
    "again, don't forget to add\n",
    "\n",
    ">import matplotlib.pyplot as plt\n",
    ">\n",
    ">%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's even scary how easy it is, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the scatter plot? It won't be as direct, you will have to use the pyplot one (the one you used last time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do that version with the colors. As last time, you need to create a new vector with the minutes since midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it may sound redundant, it may be useful to add this new series to the DataFrame (as a new column, in practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's do the graph with the colors. \n",
    "\n",
    "Remember, imagine that the number of minutes since midnight (that you just created) corresponds to a color. The function scatter allows you to give this list straight away and plot it (just use the argument c, for example \"c=my_minute_since_midnight_list\". \n",
    "\n",
    "Do you want to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the 24-hr average plot, where the x axis is 0 to 1440 (1440=24 hours X 60 minutes), and you show the average per minute.\n",
    "\n",
    "(don't forget to add the 5 and 95 quantiles) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small tip (mean of 15 minutes of the day):\n",
    "\n",
    "> df.loc[df['minute_of_day']==15].mean()\n",
    ">\n",
    ">\n",
    "Output:\n",
    "\n",
    "pickups          205.344047\n",
    "\n",
    "minute_of_day     15.000000\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "\n",
    "**IMPORTANT:** notice that the output above is not just the mean of the _pickups_ (which is what we want). It also returns the mean of the minute_of\\_day itself (which is obviously 15!)... This may confuse things later on, so how do you get the mean of pickups only?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For this part, we'll keep the text as is. The exercises are exactly the same. _\n",
    "\n",
    "One very important task in Data Science modeling is to find (and understand) correlations between different variables. Let's do a few simple exercises.\n",
    "\n",
    "Let's start with a simple question: are the different areas correlated between them? If yes, it may be interesting knowledge. For example, maybe we can share data between them to predict better, later.\n",
    "\n",
    "\n",
    "Tip: Try to make a single DataFrame with ALL time series (s1, s17, s21, s28). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_read_csv(file):\n",
    "    df = pd.read_csv(file, sep=\",\", parse_dates={'dt': [0, 1, 2]})\n",
    "    df['datetime'] = df['dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H %M\"))\n",
    "    del df['dt']\n",
    "    df = df.set_index('datetime')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = my_read_csv('data/pickups_zone_1_15min.csv')\n",
    "s17 = my_read_csv('data/pickups_zone_17_15min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge = s1.join(s17, rsuffix=\"17\")\n",
    "smerge['pickups1'] = smerge['pickups']\n",
    "del smerge['pickups']\n",
    "smerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge.corr()#['pickups1']['pickups17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a more interesting question: are there correlations between a given area, and the other areas in earlier time steps? \n",
    "\n",
    "This is a VERY important one. If you find high correlation, for example, between area 1 at time t, with area 17 at time t-1, then you can use area 17 to predict area 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check this, you need to play a little bit with the vectors. Let's call a vector that is shifted in time for 1 time step, a \"lag1\" vector. \n",
    "\n",
    "The process is similar to Numpy's, but there's a few tweaks. To make things simpler, we copy a solution from Stackoverflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLaggedFeatures(s, columns, lag=1, dropna=True):\n",
    "    '''\n",
    "    From http://stackoverflow.com/questions/20410312/how-to-create-a-lagged-data-structure-using-pandas-dataframe\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for c in s.columns:\n",
    "            new_dict[c]=s[c]\n",
    "        for col_name in columns:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create lagged Series\n",
    "            for l in range(1,lag+1):\n",
    "                new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check those correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! Very interesting!! This means that you can use data from these other areas to predict for area 1... This is useful when there is missing data in area 1, for example... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get to the autocorrelogram. Well, in Pandas, this is trivial!\n",
    "\n",
    "You can use the auto_corr method:\n",
    "\n",
    ">s1=df['pickups'] #s1 is now a Series\n",
    ">\n",
    ">print(s1.autocorr(1))  #gives the autocorrelation of lag 1\n",
    "\n",
    "Output:\n",
    "\n",
    "0.97670270955196936\n",
    "\n",
    "\n",
    "Cool, now you just need to get values for the different lags (1, 2, 3, 4...) and plot the values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables, combining datasets\n",
    "\n",
    "Ok, you're almost ready to start rockin' with actual Data Science learning :) \n",
    "\n",
    "There's a last thing before that. Quite often, the data that you get is not in numerical form. Two obvious examples are time partitioning, like weekdays (\"Monday\", \"Tuesday\"...) or time of day (\"Morning rush hour\", \"lunch time\"). These are in fact words, how can we use them in our modeling, if in practice it always requires numerical quantities?\n",
    "\n",
    "Particularly with mobility data, the examples above are quite common. For example, the time of day (rush hour VS low demand times) can be crucial for prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a new column in our dataframe with the day of week (we'll do it for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smerge['weekday']=[d.weekday() for d in smerge.index]\n",
    "#for each element in the index use the \"weekday\" function \n",
    "#(remember that the index is the datetime series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that it worked just take a look at the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, these are already numbers, but wait!... Do those quantities mean actually _something_ or aren't they just individual symbols for the week days?\n",
    "\n",
    "Other ways to respond to this question: if instead of these numbers you used others (e.g. starting at 1 instead of 0?) wouldn't the result be the same? If you use algebra on it, does the result make sense (Thursday-Tuesday=Wednesday...). \n",
    "\n",
    "The bottom line is that, in general, you shouldn't use these numbers directly in a regression model because week days do not correspond to quantities. This is a **categorical** variable, which can have one of a finite set of values. In our case:\n",
    "\n",
    "_weekday_ $\\in $ {Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's convert those values into something useful. A first obvious thing to do is to get whether a day is a week day or weekend. Let's define a simple function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_weekend(weekday):\n",
    "    return weekday in [5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test it...\n",
    "\n",
    "Another nice thing will be to actually put names there. Here's another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weekday_word(weekday):\n",
    "    if weekday==0:\n",
    "        return \"Monday\"\n",
    "    elif weekday==1:\n",
    "        return \"Tuesday\"\n",
    "    elif weekday==2:\n",
    "        return \"Wednesday\"\n",
    "    elif weekday==3:\n",
    "        return \"Thursday\"\n",
    "    elif weekday==4:\n",
    "        return \"Friday\"\n",
    "    elif weekday==5:\n",
    "        return \"Saturday\"\n",
    "    else:\n",
    "        return \"Sunday\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the new columns in our DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smerge['is_weekend'] = smerge['weekday'].apply(is_weekend)\n",
    "smerge['wd'] = [weekday_word(d) for d in smerge['weekday']]\n",
    "smerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the new DataFrame. Seems more useful, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the \"time of day\" information. Again, we provide you with the corresponding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_of_day(hour):\n",
    "    if hour<7:\n",
    "        return \"night\"\n",
    "    elif hour<9:\n",
    "        return \"morning rush\"\n",
    "    elif hour<12:\n",
    "        return \"morning\"\n",
    "    elif hour<14:\n",
    "        return \"lunch time\"\n",
    "    elif hour<17:\n",
    "        return \"afternoon\"\n",
    "    elif hour<20:\n",
    "        return \"afternoon rush\"\n",
    "    elif hour<23:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "    \n",
    "# In fact, we created another function that has the SAME functionality as above just to show you how compact and elegant \n",
    "# Python can be. \n",
    "# Don't worry, you're not expected to reach this level before of a lot of experience. \n",
    "def time_of_day2(hour):    \n",
    "    h_thresholds=[7,8,12,14,17,20,23]\n",
    "    h_names=[\"night\", \"morning rush\", \"morning\", \"lunch time\", \"afternoon\", \"afternoon rush\", \"evening\", \"night\"]\n",
    "    return h_names[next(x[0] for x in enumerate(h_thresholds) if x[1] >hour)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you create the new column with time of day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other **very important** technique is to transform categorical variables into \"dummy variables\". A dummy variable is typically binary (so it's either 1/0, or True/False), and corresponds to one single possible categorical value. \n",
    "\n",
    "**So, you transform a single variable with N different values **\n",
    "\n",
    "_time of day -> {night, morning rush, morning...evening}_\n",
    "\n",
    "**into N dummy variables, each one with 2 values (1 or 0):**\n",
    "\n",
    "_night -> {0, 1}_\n",
    "\n",
    "_morning rush -> {0, 1}_\n",
    "\n",
    "_..._\n",
    "\n",
    "_evening -> {0, 1}_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we know it sounds like hard work, but in Pandas, it's all done trivially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm_dummies=pd.get_dummies(smerge, columns=['tod'])  #this creates dummies for the \"time_of_day\" variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a well deserved reward for you. Just run the method describe() in your DataFrame and enjoy!... \n",
    "\n",
    ">s.describe()\n",
    ">\n",
    "\n",
    "Output:\n",
    "\n",
    "_something useful_  :-)\n",
    "\n",
    "\n",
    "Think about how d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sm_dummies.to_csv(\"filename\") # save dummies to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Homework: Data Fusion - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation** (or data preprocessing) in this context means manipulation of data into a form suitable for further analysis and processing. It is a process that involves many different tasks and which cannot be fully automated. Many of the data preparation activities are routine, tedious, and time consuming. It has been estimated that data preparation accounts for 60%-80% of the time spent on a data mining project!\n",
    "\n",
    "**Data fusion** is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source. \n",
    "\n",
    "In this notebook, we are going to work again with the NYC Taxi dataset. We will add a second interesting dataset with weather information, and we will try to produce more useful information for our already formulated prediction models (Week 3 Lecture).\n",
    "\n",
    "*Note: We will also use a new visualization library called \"seaborn\". You will need to install the package \"seaborn\" from anaconda's navigator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "# matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Python visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "import calendar\n",
    "\n",
    "import numpy\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the previous lecture we formulated and used a function that reads the necessary csv files at a proper format. We will use it again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_read_csv(file):\n",
    "    df = pd.read_csv(file, sep=',', parse_dates={'dt': [0, 1, 2]})\n",
    "    df['datetime']=df['dt'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H %M\"))\n",
    "    del df['dt']\n",
    "    return df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to change the path below based on where you have chosen to save your files\n",
    "s1=my_read_csv(\"data/pickups_zone_1_15min.csv\")\n",
    "s17=my_read_csv(\"data/pickups_zone_17_15min.csv\")\n",
    "s21=my_read_csv(\"data/pickups_zone_21_15min.csv\")\n",
    "s28=my_read_csv(\"data/pickups_zone_28_15min.csv\")\n",
    "\n",
    "smerge=s1.join(s17, rsuffix=\"17\")\n",
    "smerge=smerge.join(s21, rsuffix=\"21\")\n",
    "smerge=smerge.join(s28, rsuffix=\"28\")\n",
    "smerge['pickups1']=smerge['pickups']\n",
    "del smerge['pickups']\n",
    "smerge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class's notebook, we successfully found some interesting correlations for example, between area 1 at time t, with area 17 at time t-1. We will use them again here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution from Stackoverflow as used before\n",
    "def buildLaggedFeatures(s,columns, lag=1, dropna=True):\n",
    "    '''\n",
    "    From http://stackoverflow.com/questions/20410312/how-to-create-a-lagged-data-structure-using-pandas-dataframe\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for c in s.columns:\n",
    "            new_dict[c]=s[c]\n",
    "        for col_name in columns:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create lagged Series\n",
    "            for l in range(1,lag+1):\n",
    "                new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print('Only works for DataFrame or Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smerged_lagged_s1=buildLaggedFeatures(smerge, ['pickups1', 'pickups17', 'pickups21', 'pickups28'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerged_lagged_s1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "\n",
    "This public dataset was created by the National Oceanic and Atmospheric Administration (NOAA) and includes global data obtained from the USAF Climatology Center. This dataset covers GSOD data between 1929 and 2016, collected from over 9000 stations.\n",
    "\n",
    "https://cloud.google.com/bigquery/public-data/noaa-gsod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfWeather=pd.read_csv(\"data/gsod2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfWeather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes weather data from the whole world. We should find a way to select only the weather data referring to Manhattan.\n",
    "\n",
    "As you can see from the list of attributes above, there is a key that could help us select what we need; the **stn** key, that represents the station ID from which each record has been collected. \n",
    "\n",
    "But since an ID number does not allow us to know to which area it refers to, we will use a second dataset with the list of all stations and their location details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations=pd.read_csv(\"data/stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see cleary which stations refer to Manhattan, therefore we will filter the dataset using the lat-lon columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Isolation using lat-lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selection of values based on openstreetmap website export tools\n",
    "# http://www.openstreetmap.org/export#map=11/40.7504/-73.9359\n",
    "\n",
    "minlong = -74.04\n",
    "minlat = 40.69\n",
    "maxlong = -73.91\n",
    "maxlat = 40.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masklong = (stations['lon'] >= minlong) & (stations['lon'] <= maxlong)\n",
    "stationsLon = (stations.loc[masklong])\n",
    "\n",
    "masklat = (stationsLon['lat'] >= minlat) & (stationsLon['lat'] <= maxlat)\n",
    "stationsLonLat = (stationsLon.loc[masklat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we filtered the dataset, let's see how many stations we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stationsLonLat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... Something goes wrong here... Let's see how many stations are actually located on Earth!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minlong1 = -180\n",
    "minlat1 = -90\n",
    "maxlong1 = 180\n",
    "maxlat1 = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masklong = (stations['lon'] >= minlong1) & (stations['lon'] <= maxlong1)\n",
    "stationsLon1 = (stations.loc[masklong])\n",
    "\n",
    "masklat = (stationsLon1['lat'] >= minlat1) &(stationsLon1['lat'] <= maxlat1)\n",
    "stationsLonLat1 = (stationsLon1.loc[masklat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stationsLonLat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stationsLonLat1.lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 18 stations have reasonable lat-lon values... Therefore, we cannot do much using those attributes. Let's see if we can use the column **state**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.loc[stations['state']=='NY'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are stations in NY. Can you see which is the problem and why we could not see them before?\n",
    "\n",
    "It seems that the information of decimals from the lat-lon columns has been lost. We can restore it by making the following division:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations.loc[:,'lat'] /= 1000\n",
    "stations.loc[:,'lon'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.loc[stations['state']=='NY'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's use again our lat-lon filter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masklong = (stations['lon'] >= minlong) & (stations['lon'] <= maxlong)\n",
    "stationsLon = (stations.loc[masklong])\n",
    "\n",
    "masklat = (stationsLon['lat'] >= minlat) &(stationsLon['lat'] <= maxlat)\n",
    "stationsLonLat = (stationsLon.loc[masklat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsLonLat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in total we have 8 weather stations in the area of Manhattan. By examining again the header of the weather database, we can see that there is a common key between them and the stations dataset; the **\"wban\"**. So our next goal is to isolate the weather data for the stations that we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsLonLat.wban.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbanNY = stationsLonLat.wban.unique()[:2]\n",
    "print(wbanNY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherNY = dfWeather.loc[dfWeather['wban'].isin(wbanNY)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's keep in a new dataframe only the attributes that we believe could be useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY = weatherNY[['wban','year','mo','da','temp','prcp','sndp','fog','rain_drizzle']]\n",
    "weatherNY.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks normal, except sndp (check quantiles). Since the value \"999.9\" does not give any information, we will replace it with \"NaN\" (In computing, NaN, standing for *not a number*, is a numeric data type value representing an undefined or unrepresentable value.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY.loc[weatherNY.sndp == 999.9, 'sndp'] = nan\n",
    "weatherNY.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY.temp.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY.prcp.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms above, we can verify that our dataset has a normal range of temperature and precipitation values.\n",
    "\n",
    "So now let's create some new plots using the seaborn library! \n",
    "\n",
    "First, I would like to add a new column in my current dataframe with the name of the month. The column **\"mo\"** has already the number of the month, so I will use the **\"calendar.month_abbr\"** for the correspondence of each number in the name of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monthList = []\n",
    "\n",
    "for i in range(len(weatherNY)):\n",
    "    monthList.append(calendar.month_abbr[weatherNY.mo[weatherNY.index[i]]])\n",
    "\n",
    "weatherNY[\"month\"] = monthList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherNY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_weather_stats(dataset, variable, ticks):\n",
    "    # Initialize a grid of plots with an Axes for each walk\n",
    "    grid = sns.FacetGrid(dataset,col=\"month\", hue=\"month\", col_wrap=4, size=3)\n",
    "\n",
    "    # Draw a line plot to show the trajectory of each random walk\n",
    "    grid.map(plt.plot, \"da\", variable, marker=\"o\") \n",
    "\n",
    "    # Adjust the tick positions and labels\n",
    "    grid.set(yticks=ticks)\n",
    "\n",
    "    # Adjust the arrangement of the plots\n",
    "    grid.fig.tight_layout(w_pad=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose to plot the precipitation values within the range [0,3]\n",
    "show_weather_stats(weatherNY, \"prcp\", [0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each plot has lines that unites randomly the various points. This usually happens when our dataset is not sorted chronologically. Our next goal is to modify records' order based on the date they refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateList = []\n",
    "\n",
    "for i in range(len(weatherNY)):\n",
    "    date1 = datetime.date(year=weatherNY.year[weatherNY.index[i]],day=weatherNY.da[weatherNY.index[i]],month=weatherNY.mo[weatherNY.index[i]])\n",
    "    #date2 = datetime.datetime.combine(date1,datetime.time())\n",
    "    dateList.append(date1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a new column with the date\n",
    "weatherNY[\"date\"] = dateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort values based on the date\n",
    "weatherNYSorted = weatherNY.sort_values(by='date')\n",
    "weatherNYSorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the plot\n",
    "show_weather_stats(weatherNYSorted, \"prcp\", [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot for the snow \n",
    "show_weather_stats(weatherNYSorted, \"sndp\", [0, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, make a plot for the temperature values\n",
    "show_weather_stats(weatherNYSorted, \"temp\", [0, 10, 20, 40, 60, 80, 90, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weather information to our initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column \"date\" in the initial dataset (with lags)\n",
    "smerged_lagged_s1['date']=[d.date() for d in smerged_lagged_s1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smerged_lagged_s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove index (but keep the datetime information) so that the merging process will be more clear later.\n",
    "smerged_lagged_s1=smerged_lagged_s1.reset_index()\n",
    "smerged_lagged_s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge two datasets using the \"date\" as the key \n",
    "result=pd.merge(smerged_lagged_s1, weatherNYSorted, on='date', how='inner') # how: inner -> use intersection of keys from both frames, similar to a SQL inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set again the datetime as index\n",
    "result=result.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the final result \n",
    "result.to_csv('NYC_taxi_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, weather information has been included in our dataset (for 2016 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation of 2016 Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaxiData2016 = result.iloc[result.index.year == 2016]\n",
    "TaxiData2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaxiData2016.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
